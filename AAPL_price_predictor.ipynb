{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattJBorowski1991/AAPL_LSTM_simple/blob/main/AAPL_price_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JflbzNtjKBWK",
        "outputId": "82596116-6a76-47b9-b618-55e31c9c952f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFuMKcONTBHm",
        "outputId": "c6f176ec-fd20-4595-ba7d-f368e2dda743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Apple stock data from Yahoo Finance...\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (11182, 5)\n",
            "Date range: 1980-12-12 00:00:00 to 2025-04-24 00:00:00\n",
            "Total days: 11182\n",
            "Total datapoints: 55910\n",
            "\n",
            "Data Preview:\n",
            "Price          Close      High       Low      Open     Volume\n",
            "Ticker          AAPL      AAPL      AAPL      AAPL       AAPL\n",
            "Date                                                         \n",
            "1980-12-12  0.098726  0.099155  0.098726  0.098726  469033600\n",
            "1980-12-15  0.093575  0.094005  0.093575  0.094005  175884800\n",
            "1980-12-16  0.086707  0.087136  0.086707  0.087136  105728000\n",
            "1980-12-17  0.088853  0.089282  0.088853  0.088853   86441600\n",
            "1980-12-18  0.091429  0.091858  0.091429  0.091429   73449600\n",
            "\n",
            "Missing values:\n",
            "Price   Ticker\n",
            "Close   AAPL      0\n",
            "High    AAPL      0\n",
            "Low     AAPL      0\n",
            "Open    AAPL      0\n",
            "Volume  AAPL      0\n",
            "dtype: int64\n",
            "\n",
            "Calculating technical indicators...\n",
            "Original features: 5\n",
            "Expanded features: 12\n",
            "Total datapoints after feature engineering: 134016\n",
            "\n",
            "Available features in the dataset:\n",
            "1. ('Close', 'AAPL')\n",
            "2. ('High', 'AAPL')\n",
            "3. ('Low', 'AAPL')\n",
            "4. ('Open', 'AAPL')\n",
            "5. ('Volume', 'AAPL')\n",
            "6. ('RSI_14', '')\n",
            "7. ('Day_of_Week', '')\n",
            "8. ('Month', '')\n",
            "9. ('Day_of_Month', '')\n",
            "10. ('Gap_Up', '')\n",
            "11. ('Gap_Down', '')\n",
            "12. ('Next_Day_Open', '')\n",
            "\n",
            "Preparing data for training...\n",
            "Applying MinMax scaling to features...\n",
            "Creating sequences with lookback period of 90 days...\n",
            "Sequence shape: (11078, 90, 11)\n",
            "Target shape: (11078,)\n",
            "Training set: (9970, 90, 11)\n",
            "Testing set: (1108, 90, 11)\n",
            "\n",
            "Data preparation complete! Files saved and ready for model training.\n",
            "Feature list saved to 'feature_list.txt'\n"
          ]
        }
      ],
      "source": [
        "# Apple Stock Price Prediction - Data Preparation\n",
        "# This script downloads Apple stock data and prepares it for deep learning\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Download Apple's complete stock history\n",
        "print(\"Downloading Apple stock data from Yahoo Finance...\")\n",
        "aapl_data = yf.download(\"AAPL\", period=\"max\")\n",
        "\n",
        "# Display info about the dataset\n",
        "print(f\"Dataset shape: {aapl_data.shape}\")\n",
        "print(f\"Date range: {aapl_data.index.min()} to {aapl_data.index.max()}\")\n",
        "print(f\"Total days: {aapl_data.shape[0]}\")\n",
        "print(f\"Total datapoints: {aapl_data.shape[0] * aapl_data.shape[1]}\")\n",
        "\n",
        "# Preview the data\n",
        "print(\"\\nData Preview:\")\n",
        "print(aapl_data.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(aapl_data.isnull().sum())\n",
        "\n",
        "# Calculate technical indicators\n",
        "print(\"\\nCalculating technical indicators...\")\n",
        "df = aapl_data.copy()\n",
        "\n",
        "# 1. RSI (14-day)\n",
        "delta = df['Close'].diff()\n",
        "gain = delta.where(delta > 0, 0)\n",
        "loss = -delta.where(delta < 0, 0)\n",
        "avg_gain = gain.rolling(window=14).mean()\n",
        "avg_loss = loss.rolling(window=14).mean()\n",
        "rs = avg_gain / avg_loss\n",
        "df['RSI_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "# 2. Add date-based features\n",
        "df['Day_of_Week'] = df.index.dayofweek\n",
        "df['Month'] = df.index.month\n",
        "df['Day_of_Month'] = df.index.day\n",
        "\n",
        "# 3. Price patterns - gaps\n",
        "df['Gap_Up'] = ((df['Open'] > df['Close'].shift(1)) * 1)\n",
        "df['Gap_Down'] = ((df['Open'] < df['Close'].shift(1)) * 1)\n",
        "\n",
        "# 4. Add our target variable - next day's opening price\n",
        "df['Next_Day_Open'] = df['Open'].shift(-1)\n",
        "\n",
        "# Remove rows with NaN values (from rolling calculations)\n",
        "df_clean = df.dropna()\n",
        "print(f\"Original features: {aapl_data.shape[1]}\")\n",
        "print(f\"Expanded features: {df.shape[1]}\")\n",
        "print(f\"Total datapoints after feature engineering: {df_clean.shape[0] * df_clean.shape[1]}\")\n",
        "\n",
        "# Show all features\n",
        "print(\"\\nAvailable features in the dataset:\")\n",
        "for i, col in enumerate(df_clean.columns):\n",
        "    print(f\"{i+1}. {col}\")\n",
        "\n",
        "# Prepare data for model training\n",
        "print(\"\\nPreparing data for training...\")\n",
        "\n",
        "# Define features and target\n",
        "X = df_clean.drop(['Next_Day_Open'], axis=1)\n",
        "y = df_clean['Next_Day_Open']\n",
        "\n",
        "# Feature scaling\n",
        "print(\"Applying MinMax scaling to features...\")\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Create sequences for time series prediction\n",
        "def create_sequences(X, y, time_steps=90):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        X_seq.append(X[i:i + time_steps])\n",
        "        y_seq.append(y[i + time_steps])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Define sequence length (lookback period)\n",
        "sequence_length = 90  # Using 90 days of data to predict the next day\n",
        "\n",
        "print(f\"Creating sequences with lookback period of {sequence_length} days...\")\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_scaled, sequence_length)\n",
        "\n",
        "print(f\"Sequence shape: {X_seq.shape}\")\n",
        "print(f\"Target shape: {y_seq.shape}\")\n",
        "\n",
        "# Train-test split (90-10)\n",
        "train_size = int(len(X_seq) * 0.9)\n",
        "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
        "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}\")\n",
        "\n",
        "# Save processed data\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_test.npy', y_test)\n",
        "\n",
        "# Save scalers for later use\n",
        "import pickle\n",
        "with open('scaler_X.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_X, f)\n",
        "with open('scaler_y.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_y, f)\n",
        "\n",
        "print(\"\\nData preparation complete! Files saved and ready for model training.\")\n",
        "\n",
        "# Save the feature list for reference\n",
        "with open('feature_list.txt', 'w') as f:\n",
        "    for feature in X.columns:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "\n",
        "print(\"Feature list saved to 'feature_list.txt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhjtiOltix6U",
        "outputId": "3a9af444-b851-4255-f2f4-c508773346d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model 1/10\n",
            "Epoch 1/10, Loss: 0.035424\n",
            "Epoch 2/10, Loss: 0.001720\n",
            "Epoch 3/10, Loss: 0.000592\n",
            "Epoch 4/10, Loss: 0.000395\n",
            "Epoch 5/10, Loss: 0.000275\n",
            "Epoch 6/10, Loss: 0.000281\n",
            "Epoch 7/10, Loss: 0.000271\n",
            "Epoch 8/10, Loss: 0.000229\n",
            "Epoch 9/10, Loss: 0.000264\n",
            "Epoch 10/10, Loss: 0.000223\n",
            "Test Loss: 0.000423\n",
            "\n",
            "Training model 2/10\n",
            "Epoch 1/10, Loss: 0.024402\n",
            "Epoch 2/10, Loss: 0.000862\n",
            "Epoch 3/10, Loss: 0.000376\n",
            "Epoch 4/10, Loss: 0.000276\n",
            "Epoch 5/10, Loss: 0.000261\n",
            "Epoch 6/10, Loss: 0.000273\n",
            "Epoch 7/10, Loss: 0.000221\n",
            "Epoch 8/10, Loss: 0.000254\n",
            "Epoch 9/10, Loss: 0.000252\n",
            "Epoch 10/10, Loss: 0.000256\n",
            "Test Loss: 0.000575\n",
            "\n",
            "Training model 3/10\n",
            "Epoch 1/20, Loss: 0.023465\n",
            "Epoch 2/20, Loss: 0.001012\n",
            "Epoch 3/20, Loss: 0.000372\n",
            "Epoch 4/20, Loss: 0.000305\n",
            "Epoch 5/20, Loss: 0.000298\n",
            "Epoch 6/20, Loss: 0.000258\n",
            "Epoch 7/20, Loss: 0.000278\n",
            "Epoch 8/20, Loss: 0.000239\n",
            "Epoch 9/20, Loss: 0.000260\n",
            "Epoch 10/20, Loss: 0.000258\n",
            "Epoch 11/20, Loss: 0.000283\n",
            "Epoch 12/20, Loss: 0.000268\n",
            "Epoch 13/20, Loss: 0.000272\n",
            "Epoch 14/20, Loss: 0.000302\n",
            "Epoch 15/20, Loss: 0.000290\n",
            "Epoch 16/20, Loss: 0.000378\n",
            "Epoch 17/20, Loss: 0.000289\n",
            "Epoch 18/20, Loss: 0.000278\n",
            "Epoch 19/20, Loss: 0.000323\n",
            "Epoch 20/20, Loss: 0.000298\n",
            "Test Loss: 0.002980\n",
            "\n",
            "Training model 4/10\n",
            "Epoch 1/10, Loss: 0.022238\n",
            "Epoch 2/10, Loss: 0.000956\n",
            "Epoch 3/10, Loss: 0.000456\n",
            "Epoch 4/10, Loss: 0.000329\n",
            "Epoch 5/10, Loss: 0.000287\n",
            "Epoch 6/10, Loss: 0.000279\n",
            "Epoch 7/10, Loss: 0.000254\n",
            "Epoch 8/10, Loss: 0.000261\n",
            "Epoch 9/10, Loss: 0.000258\n",
            "Epoch 10/10, Loss: 0.000258\n",
            "Test Loss: 0.001078\n",
            "\n",
            "Training model 5/10\n",
            "Epoch 1/10, Loss: 0.020246\n",
            "Epoch 2/10, Loss: 0.000883\n",
            "Epoch 3/10, Loss: 0.000433\n",
            "Epoch 4/10, Loss: 0.000310\n",
            "Epoch 5/10, Loss: 0.000264\n",
            "Epoch 6/10, Loss: 0.000276\n",
            "Epoch 7/10, Loss: 0.000249\n",
            "Epoch 8/10, Loss: 0.000258\n",
            "Epoch 9/10, Loss: 0.000259\n",
            "Epoch 10/10, Loss: 0.000259\n",
            "Test Loss: 0.000525\n",
            "\n",
            "Training model 6/10\n",
            "Epoch 1/15, Loss: 0.029111\n",
            "Epoch 2/15, Loss: 0.001305\n",
            "Epoch 3/15, Loss: 0.000473\n",
            "Epoch 4/15, Loss: 0.000297\n",
            "Epoch 5/15, Loss: 0.000253\n",
            "Epoch 6/15, Loss: 0.000234\n",
            "Epoch 7/15, Loss: 0.000255\n",
            "Epoch 8/15, Loss: 0.000233\n",
            "Epoch 9/15, Loss: 0.000232\n",
            "Epoch 10/15, Loss: 0.000240\n",
            "Epoch 11/15, Loss: 0.000233\n",
            "Epoch 12/15, Loss: 0.000218\n",
            "Epoch 13/15, Loss: 0.000243\n",
            "Epoch 14/15, Loss: 0.000257\n",
            "Epoch 15/15, Loss: 0.000268\n",
            "Test Loss: 0.000434\n",
            "\n",
            "Training model 7/10\n",
            "Epoch 1/10, Loss: 0.036695\n",
            "Epoch 2/10, Loss: 0.001547\n",
            "Epoch 3/10, Loss: 0.000523\n",
            "Epoch 4/10, Loss: 0.000335\n",
            "Epoch 5/10, Loss: 0.000305\n",
            "Epoch 6/10, Loss: 0.000260\n",
            "Epoch 7/10, Loss: 0.000258\n",
            "Epoch 8/10, Loss: 0.000247\n",
            "Epoch 9/10, Loss: 0.000269\n",
            "Epoch 10/10, Loss: 0.000239\n",
            "Test Loss: 0.000398\n",
            "\n",
            "Training model 8/10\n",
            "Epoch 1/15, Loss: 0.031950\n",
            "Epoch 2/15, Loss: 0.001407\n",
            "Epoch 3/15, Loss: 0.000534\n",
            "Epoch 4/15, Loss: 0.000345\n",
            "Epoch 5/15, Loss: 0.000308\n",
            "Epoch 6/15, Loss: 0.000255\n",
            "Epoch 7/15, Loss: 0.000248\n",
            "Epoch 8/15, Loss: 0.000230\n",
            "Epoch 9/15, Loss: 0.000221\n",
            "Epoch 10/15, Loss: 0.000250\n",
            "Epoch 11/15, Loss: 0.000233\n",
            "Epoch 12/15, Loss: 0.000251\n",
            "Epoch 13/15, Loss: 0.000235\n",
            "Epoch 14/15, Loss: 0.000269\n",
            "Epoch 15/15, Loss: 0.000260\n",
            "Test Loss: 0.000480\n",
            "\n",
            "Training model 9/10\n",
            "Epoch 1/10, Loss: 0.024753\n",
            "Epoch 2/10, Loss: 0.001139\n",
            "Epoch 3/10, Loss: 0.000451\n",
            "Epoch 4/10, Loss: 0.000304\n",
            "Epoch 5/10, Loss: 0.000277\n",
            "Epoch 6/10, Loss: 0.000226\n",
            "Epoch 7/10, Loss: 0.000249\n",
            "Epoch 8/10, Loss: 0.000230\n",
            "Epoch 9/10, Loss: 0.000232\n",
            "Epoch 10/10, Loss: 0.000236\n",
            "Test Loss: 0.000548\n",
            "\n",
            "Training model 10/10\n",
            "Epoch 1/15, Loss: 0.022884\n",
            "Epoch 2/15, Loss: 0.000851\n",
            "Epoch 3/15, Loss: 0.000407\n",
            "Epoch 4/15, Loss: 0.000293\n",
            "Epoch 5/15, Loss: 0.000287\n",
            "Epoch 6/15, Loss: 0.000259\n",
            "Epoch 7/15, Loss: 0.000262\n",
            "Epoch 8/15, Loss: 0.000221\n",
            "Epoch 9/15, Loss: 0.000275\n",
            "Epoch 10/15, Loss: 0.000276\n",
            "Epoch 11/15, Loss: 0.000233\n",
            "Epoch 12/15, Loss: 0.000291\n",
            "Epoch 13/15, Loss: 0.000288\n",
            "Epoch 14/15, Loss: 0.000253\n",
            "Epoch 15/15, Loss: 0.000292\n",
            "Test Loss: 0.001243\n",
            "\n",
            "Hyperparameter Search Results:\n",
            "   batch_size  hidden_size  num_layers  dropout_rate  learning_rate  \\\n",
            "0          32           64           2          0.25          0.001   \n",
            "1          32           64           2          0.25          0.001   \n",
            "2          32           64           2          0.25          0.001   \n",
            "3          32           64           2          0.25          0.001   \n",
            "4          32           64           2          0.25          0.001   \n",
            "5          32           64           2          0.25          0.001   \n",
            "6          32           64           2          0.25          0.001   \n",
            "7          32           64           2          0.25          0.001   \n",
            "8          32           64           2          0.25          0.001   \n",
            "9          32           64           2          0.25          0.001   \n",
            "\n",
            "   weight_decay  num_epochs  test_loss  \n",
            "0         0.001          10   0.000423  \n",
            "1         0.001          10   0.000575  \n",
            "2         0.001          20   0.002980  \n",
            "3         0.001          10   0.001078  \n",
            "4         0.001          10   0.000525  \n",
            "5         0.001          15   0.000434  \n",
            "6         0.001          10   0.000398  \n",
            "7         0.001          15   0.000480  \n",
            "8         0.001          10   0.000548  \n",
            "9         0.001          15   0.001243  \n",
            "\n",
            "Best Model Configuration:\n",
            "batch_size       32.000000\n",
            "hidden_size      64.000000\n",
            "num_layers        2.000000\n",
            "dropout_rate      0.250000\n",
            "learning_rate     0.001000\n",
            "weight_decay      0.001000\n",
            "num_epochs       10.000000\n",
            "test_loss         0.000398\n",
            "Name: 6, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# ----- ADDED IMPORTS FOR HYPERPARAMETER SEARCH -----\n",
        "import random\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Load prepared data\n",
        "X_train = np.load('X_train.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "# Define custom dataset class\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx].unsqueeze(-1)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = StockDataset(X_train, y_train)\n",
        "test_dataset = StockDataset(X_test, y_test)\n",
        "\n",
        "def create_data_loaders(batch_size):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Define model architecture\n",
        "class StockPredictor(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, dropout_rate):\n",
        "        super(StockPredictor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.bn = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.bn(out[:, -1, :])\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Define hyperparameter ranges\n",
        "hyperparams = {\n",
        "    'batch_size': [32],\n",
        "    'hidden_size': [64],\n",
        "    'num_layers': [2],\n",
        "    'dropout_rate': [0.25],\n",
        "    'learning_rate': [0.001],\n",
        "    'weight_decay': [0.001],\n",
        "    'num_epochs': [10, 15, 20]\n",
        "}\n",
        "\n",
        "\n",
        "# Best Model Configuration:\n",
        "\n",
        "# ITERATION 1\n",
        "# batch_size       64.000000\n",
        "# hidden_size      64.000000\n",
        "# num_layers        3.000000\n",
        "# dropout_rate      0.200000\n",
        "# learning_rate     0.001000\n",
        "# weight_decay      0.001000\n",
        "# num_epochs       15.000000\n",
        "# test_loss         0.000956\n",
        "\n",
        "# ITERATION 2\n",
        "# Best Model Configuration:\n",
        "# batch_size       32.000000\n",
        "# hidden_size      64.000000\n",
        "# num_layers        2.000000\n",
        "# dropout_rate      0.250000\n",
        "# learning_rate     0.001000\n",
        "# weight_decay      0.001000\n",
        "# num_epochs        7.000000\n",
        "# test_loss         0.000447\n",
        "# Name: 4, dtype: float64\n",
        "# 2nd Best Model Configuration:\n",
        "# batch_size       128.000000\n",
        "# hidden_size       64.000000\n",
        "# num_layers         3.000000\n",
        "# dropout_rate       0.250000\n",
        "# learning_rate      0.001000\n",
        "# weight_decay       0.000100\n",
        "# num_epochs         6.000000\n",
        "# test_loss          0.000491\n",
        "# Name: 3, dtype: float64\n",
        "\n",
        "# ITERATION 3 (inferior to ITERATION 2. Go back)\n",
        "# Best Model Configuration:\n",
        "# batch_size       64.000000\n",
        "# hidden_size      64.000000\n",
        "# num_layers        2.000000\n",
        "# dropout_rate      0.300000\n",
        "# learning_rate     0.001000\n",
        "# weight_decay      0.001000\n",
        "# num_epochs       10.000000\n",
        "# test_loss         0.000512\n",
        "# Name: 8, dtype: float64\n",
        "\n",
        "# ITERATION 4 (Slightly better than ITERATION 2. Train more.)\n",
        "# Best Model Configuration:\n",
        "# batch_size       32.000000\n",
        "# hidden_size      64.000000\n",
        "# num_layers        2.000000\n",
        "# dropout_rate      0.250000\n",
        "# learning_rate     0.001000\n",
        "# weight_decay      0.001000\n",
        "# num_epochs        8.000000\n",
        "# test_loss         0.000401\n",
        "# Name: 3, dtype: float64\n",
        "\n",
        "# ITERATION 5\n",
        "# Best Model Configuration:\n",
        "# batch_size       32.000000\n",
        "# hidden_size      64.000000\n",
        "# num_layers        2.000000\n",
        "# dropout_rate      0.250000\n",
        "# learning_rate     0.001000\n",
        "# weight_decay      0.001000\n",
        "# num_epochs       10.000000\n",
        "# test_loss         0.000398\n",
        "# Name: 6, dtype: float64\n",
        "\n",
        "\n",
        "# Number of random samples to try\n",
        "num_samples = 10\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Random sampling loop\n",
        "for i in range(num_samples):\n",
        "    print(f\"\\nTraining model {i+1}/{num_samples}\")\n",
        "\n",
        "    # Randomly sample hyperparameters\n",
        "    config = {\n",
        "        'batch_size': random.choice(hyperparams['batch_size']),\n",
        "        'hidden_size': random.choice(hyperparams['hidden_size']),\n",
        "        'num_layers': random.choice(hyperparams['num_layers']),\n",
        "        'dropout_rate': random.choice(hyperparams['dropout_rate']),\n",
        "        'learning_rate': random.choice(hyperparams['learning_rate']),\n",
        "        'weight_decay': random.choice(hyperparams['weight_decay']),\n",
        "        'num_epochs': random.choice(hyperparams['num_epochs'])\n",
        "    }\n",
        "\n",
        "    # Create data loaders with sampled batch size\n",
        "    train_loader, test_loader = create_data_loaders(config['batch_size'])\n",
        "\n",
        "    # Initialize model with sampled parameters\n",
        "    model = StockPredictor(\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout_rate=config['dropout_rate']\n",
        "    )\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            X_batch, y_batch = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}, Loss: {total_loss / len(train_loader):.6f}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            X_batch, y_batch = batch\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.6f}\")\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        **config,\n",
        "        'test_loss': avg_test_loss\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame and find best model\n",
        "results_df = pd.DataFrame(results)\n",
        "best_model = results_df.loc[results_df['test_loss'].idxmin()]\n",
        "\n",
        "# Print results\n",
        "print(\"\\nHyperparameter Search Results:\")\n",
        "print(results_df)\n",
        "print(\"\\nBest Model Configuration:\")\n",
        "print(best_model)\n",
        "# -------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LPn4orVLEEDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69c53f9-7692-4b7e-9e4d-0e24970e329b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2nd Best Model Configuration:\n",
            "batch_size       32.000000\n",
            "hidden_size      64.000000\n",
            "num_layers        2.000000\n",
            "dropout_rate      0.250000\n",
            "learning_rate     0.001000\n",
            "weight_decay      0.001000\n",
            "num_epochs       10.000000\n",
            "test_loss         0.000423\n",
            "Name: 0, dtype: float64\n",
            "\n",
            "3rd Best Model Configuration:\n",
            "batch_size       32.000000\n",
            "hidden_size      64.000000\n",
            "num_layers        2.000000\n",
            "dropout_rate      0.250000\n",
            "learning_rate     0.001000\n",
            "weight_decay      0.001000\n",
            "num_epochs       15.000000\n",
            "test_loss         0.000434\n",
            "Name: 5, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "sorted_results = results_df.sort_values(by='test_loss')\n",
        "second_best_model = sorted_results.iloc[1]  # 2nd best model\n",
        "third_best_model = sorted_results.iloc[2]   # 3rd best model\n",
        "\n",
        "# Print 2nd and 3rd best models\n",
        "print(\"\\n2nd Best Model Configuration:\")\n",
        "print(second_best_model)\n",
        "print(\"\\n3rd Best Model Configuration:\")\n",
        "print(third_best_model)\n",
        "# ----------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg2tYIg1YIJYWYjoOkzlLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}