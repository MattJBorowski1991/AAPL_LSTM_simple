{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattJBorowski1991/AAPL_LSTM_simple/blob/main/AAPL_price_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JflbzNtjKBWK",
        "outputId": "6c063b54-b3af-4726-f597-20f498ca1498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Mounted at /content/drive\n",
            "Files will be saved to: /content/drive/MyDrive/Colab Notebooks/AAPL price predictor\n",
            "Write permissions verified for /content/drive/MyDrive/Colab Notebooks/AAPL price predictor\n",
            "Checking for new Apple stock data...\n",
            "Fetching new data from 2025-04-26 to 2025-04-28...\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated data saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/aapl_data.csv\n",
            "Dataset shape: (11183, 5)\n",
            "Date range: 1980-12-12 00:00:00 to 2025-04-25 00:00:00\n",
            "Total days: 11183\n",
            "Total datapoints: 55915\n",
            "\n",
            "Data Preview:\n",
            "               Close      High       Low      Open     Volume\n",
            "Date                                                         \n",
            "1980-12-12  0.098726  0.099155  0.098726  0.098726  469033600\n",
            "1980-12-15  0.093575  0.094005  0.093575  0.094005  175884800\n",
            "1980-12-16  0.086707  0.087136  0.086707  0.087136  105728000\n",
            "1980-12-17  0.088853  0.089282  0.088853  0.088853   86441600\n",
            "1980-12-18  0.091429  0.091858  0.091429  0.091429   73449600\n",
            "\n",
            "Missing values:\n",
            "Close     0\n",
            "High      0\n",
            "Low       0\n",
            "Open      0\n",
            "Volume    0\n",
            "dtype: int64\n",
            "\n",
            "Calculating technical indicators...\n",
            "Original features: 5\n",
            "Expanded features: 12\n",
            "Total datapoints after feature engineering: 134028\n",
            "\n",
            "Available features in the dataset:\n",
            "1. Close\n",
            "2. High\n",
            "3. Low\n",
            "4. Open\n",
            "5. Volume\n",
            "6. RSI_14\n",
            "7. DAY_OF_WEEK\n",
            "8. MONTH\n",
            "9. DAY_OF_MONTH\n",
            "10. Gap_Up\n",
            "11. GAP_DOWN\n",
            "12. Next_Day_Open\n",
            "\n",
            "Preparing data for training...\n",
            "Applying MinMax scaling to features...\n",
            "Creating sequences with lookback period of 90 days...\n",
            "Sequence shape: (11079, 90, 11)\n",
            "Target shape: (11079,)\n",
            "Training set: (9971, 90, 11)\n",
            "Testing set: (1108, 90, 11)\n",
            "\n",
            "Data preparation complete! Files saved and ready for model training.\n",
            "Feature list saved to 'feature_list.txt'\n"
          ]
        }
      ],
      "source": [
        "# Apple Stock Price Prediction - Data Preparation\n",
        "\n",
        "# Install required packages\n",
        "!pip install yfinance pandas matplotlib\n",
        "!pip install torch\n",
        "\n",
        "# This script downloads Apple stock data and prepares it for deep learning\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "notebook_dir = '/content/drive/MyDrive/Colab Notebooks/AAPL price predictor'  # CHANGE: Update this path to your .ipynb folder\n",
        "if not os.path.exists(notebook_dir):\n",
        "    raise ValueError(f\"Directory {notebook_dir} does not exist. Please update notebook_dir to the correct path containing your .ipynb file.\")\n",
        "print(f\"Files will be saved to: {notebook_dir}\")\n",
        "\n",
        "# CHANGE: Verify write permissions by creating a test file\n",
        "test_file = os.path.join(notebook_dir, 'test_write.txt')\n",
        "try:\n",
        "    with open(test_file, 'w') as f:\n",
        "        f.write('Test')\n",
        "    os.remove(test_file)\n",
        "    print(f\"Write permissions verified for {notebook_dir}\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Cannot write to {notebook_dir}: {e}\")\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# ADDED: Function to fetch new Apple stock data\n",
        "def fetch_new_data(last_date):\n",
        "    \"\"\"Fetch new Apple stock data starting from the day after last_date.\"\"\"\n",
        "    end_date = datetime.now().date()\n",
        "    start_date = last_date + timedelta(days=1)\n",
        "    if start_date >= end_date:\n",
        "        print(\"No new data to fetch.\")\n",
        "        return None\n",
        "    print(f\"Fetching new data from {start_date} to {end_date}...\")\n",
        "    new_data = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
        "    return new_data\n",
        "\n",
        "# ADDED: Function to check and update data\n",
        "def update_stock_data():\n",
        "    \"\"\"Check for new data and append to existing dataset if not already present.\"\"\"\n",
        "    data_file = os.path.join(notebook_dir, 'aapl_data.csv')\n",
        "\n",
        "    # Load existing data or download full history\n",
        "    if os.path.exists(data_file):\n",
        "        df = pd.read_csv(data_file, index_col='Date', parse_dates=True)\n",
        "        last_date = df.index.max().date()\n",
        "    else:\n",
        "        print(\"No existing data found. Downloading full history...\")\n",
        "        df = yf.download(\"AAPL\", period=\"max\")\n",
        "        last_date = df.index.max().date()\n",
        "\n",
        "    # Flatten MultiIndex columns if present\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [col[0] if col[1] == '' else f\"{col[0]}_{col[1]}\" for col in df.columns]\n",
        "        # For single-ticker data, remove '_AAPL' suffix for simplicity\n",
        "        df.columns = [col.replace('_AAPL', '') for col in df.columns]\n",
        "\n",
        "    # Fetch new data\n",
        "    new_data = fetch_new_data(last_date)\n",
        "\n",
        "    if new_data is not None and not new_data.empty:\n",
        "        # Flatten MultiIndex columns for new data\n",
        "        if isinstance(new_data.columns, pd.MultiIndex):\n",
        "            new_data.columns = [col[0] if col[1] == '' else f\"{col[0]}_{col[1]}\" for col in new_data.columns]\n",
        "            new_data.columns = [col.replace('_AAPL', '') for col in new_data.columns]\n",
        "        # Append new data\n",
        "        df = pd.concat([df, new_data])\n",
        "        # Remove duplicates based on date index\n",
        "        df = df[~df.index.duplicated(keep='last')]\n",
        "        # Sort by date\n",
        "        df = df.sort_index()\n",
        "        # Save updated data to Google Drive\n",
        "        df.to_csv(data_file)\n",
        "        print(f\"Updated data saved to {data_file}\")\n",
        "    else:\n",
        "        print(\"No new data to append.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Download or update Apple's stock data\n",
        "print(\"Checking for new Apple stock data...\")\n",
        "aapl_data = update_stock_data()\n",
        "\n",
        "# Display info about the dataset\n",
        "print(f\"Dataset shape: {aapl_data.shape}\")\n",
        "print(f\"Date range: {aapl_data.index.min()} to {aapl_data.index.max()}\")\n",
        "print(f\"Total days: {aapl_data.shape[0]}\")\n",
        "print(f\"Total datapoints: {aapl_data.shape[0] * aapl_data.shape[1]}\")\n",
        "\n",
        "# Preview the data\n",
        "print(\"\\nData Preview:\")\n",
        "print(aapl_data.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(aapl_data.isnull().sum())\n",
        "\n",
        "# Calculate technical indicators\n",
        "print(\"\\nCalculating technical indicators...\")\n",
        "df = aapl_data.copy()\n",
        "\n",
        "# 1. RSI (14-day)\n",
        "delta = df['Close'].diff()\n",
        "gain = delta.where(delta > 0, 0)\n",
        "loss = -delta.where(delta < 0, 0)\n",
        "avg_gain = gain.rolling(window=14).mean()\n",
        "avg_loss = loss.rolling(window=14).mean()\n",
        "rs = avg_gain / avg_loss\n",
        "df['RSI_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "# 2. Add date-based features\n",
        "df['DAY_OF_WEEK'] = df.index.dayofweek\n",
        "df['MONTH'] = df.index.month\n",
        "df['DAY_OF_MONTH'] = df.index.day\n",
        "\n",
        "# 3. Price patterns - gaps\n",
        "df['Gap_Up'] = ((df['Open'] > df['Close'].shift(1)) * 1)\n",
        "df['GAP_DOWN'] = ((df['Open'] < df['Close'].shift(1)) * 1)\n",
        "\n",
        "# 4. Add our target variable - next day's opening price\n",
        "df['Next_Day_Open'] = df['Open'].shift(-1)\n",
        "\n",
        "# Remove rows with NaN values (from rolling calculations)\n",
        "df_clean = df.dropna()\n",
        "print(f\"Original features: {aapl_data.shape[1]}\")\n",
        "print(f\"Expanded features: {df.shape[1]}\")\n",
        "print(f\"Total datapoints after feature engineering: {df_clean.shape[0] * df_clean.shape[1]}\")\n",
        "\n",
        "# Show all features\n",
        "print(\"\\nAvailable features in the dataset:\")\n",
        "for i, col in enumerate(df_clean.columns):\n",
        "    print(f\"{i+1}. {col}\")\n",
        "\n",
        "# Prepare data for model training\n",
        "print(\"\\nPreparing data for training...\")\n",
        "\n",
        "# Define features and target\n",
        "X = df_clean.drop(['Next_Day_Open'], axis=1)\n",
        "y = df_clean['Next_Day_Open']\n",
        "\n",
        "# Feature scaling\n",
        "print(\"Applying MinMax scaling to features...\")\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Create sequences for time series prediction\n",
        "def create_sequences(X, y, time_steps=90):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        X_seq.append(X[i:i + time_steps])\n",
        "        y_seq.append(y[i + time_steps])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Define sequence length (lookback period)\n",
        "sequence_length = 90  # Using 90 days of data to predict the next day\n",
        "print(f\"Creating sequences with lookback period of {sequence_length} days...\")\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_scaled, sequence_length)\n",
        "print(f\"Sequence shape: {X_seq.shape}\")\n",
        "print(f\"Target shape: {y_seq.shape}\")\n",
        "\n",
        "# Train-test split\n",
        "train_size = int(len(X_seq) * 0.9)\n",
        "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
        "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}\")\n",
        "\n",
        "# Save processed data\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_test.npy', y_test)\n",
        "\n",
        "# Save scalers for later use\n",
        "import pickle\n",
        "with open('scaler_X.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_X, f)\n",
        "with open('scaler_y.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_y, f)\n",
        "print(\"\\nData preparation complete! Files saved and ready for model training.\")\n",
        "\n",
        "# Save the feature list for reference\n",
        "with open('feature_list.txt', 'w') as f:\n",
        "    for feature in X.columns:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "print(\"Feature list saved to 'feature_list.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JFuMKcONTBHm",
        "outputId": "d0a41474-4bc2-4525-c907-e6bde43071a7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing hyperparameter search over 100 random configurations\n",
            "\n",
            "Training with config 1/100: {'batch_size': 128, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.001, 'num_epochs': 20}\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/checkpoint_best_config_0.pth has incompatible hyperparameters: {'hidden_size': 32, 'num_layers': 1, 'dropout_rate': 0.1}\n",
            "Current config: {'batch_size': 128, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.001, 'num_epochs': 20}\n",
            "Initializing model with random weights for config 0\n",
            "Training model with 20 epochs\n",
            "Config 1, Epoch 1/20, Train Loss: 0.055298, Test Loss: 0.009270\n",
            "Config 1, Epoch 2/20, Train Loss: 0.020512, Test Loss: 0.061041\n",
            "Config 1, Epoch 3/20, Train Loss: 0.013976, Test Loss: 0.004354\n",
            "Config 1, Epoch 4/20, Train Loss: 0.008802, Test Loss: 0.003934\n",
            "Config 1, Epoch 5/20, Train Loss: 0.005647, Test Loss: 0.002723\n",
            "Config 1, Epoch 6/20, Train Loss: 0.003513, Test Loss: 0.000537\n",
            "Config 1, Epoch 7/20, Train Loss: 0.002231, Test Loss: 0.000441\n",
            "Config 1, Epoch 8/20, Train Loss: 0.001393, Test Loss: 0.000970\n",
            "Config 1, Epoch 9/20, Train Loss: 0.000876, Test Loss: 0.000338\n",
            "Config 1, Epoch 10/20, Train Loss: 0.000593, Test Loss: 0.000363\n",
            "Config 1, Epoch 11/20, Train Loss: 0.000403, Test Loss: 0.000550\n",
            "Config 1, Epoch 12/20, Train Loss: 0.000284, Test Loss: 0.000471\n",
            "Config 1, Epoch 13/20, Train Loss: 0.000233, Test Loss: 0.000341\n",
            "Config 1, Epoch 14/20, Train Loss: 0.000191, Test Loss: 0.000340\n",
            "Config 1, Epoch 15/20, Train Loss: 0.000168, Test Loss: 0.000572\n",
            "Config 1, Epoch 16/20, Train Loss: 0.000147, Test Loss: 0.000761\n",
            "Config 1, Epoch 17/20, Train Loss: 0.000158, Test Loss: 0.001101\n",
            "Config 1, Epoch 18/20, Train Loss: 0.000130, Test Loss: 0.000675\n",
            "Config 1, Epoch 19/20, Train Loss: 0.000126, Test Loss: 0.000409\n",
            "Config 1, Epoch 20/20, Train Loss: 0.000143, Test Loss: 0.000514\n",
            "Model for config 1 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/stock_predictor_model_config_0.pth\n",
            "Training history for config 1 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/training_history_config_0.csv\n",
            "\n",
            "Training with config 2/100: {'batch_size': 16, 'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'weight_decay': 0.0, 'num_epochs': 10}\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/checkpoint_best_config_1.pth has incompatible hyperparameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.1}\n",
            "Current config: {'batch_size': 16, 'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'weight_decay': 0.0, 'num_epochs': 10}\n",
            "Initializing model with random weights for config 1\n",
            "Training model with 10 epochs\n",
            "Config 2, Epoch 1/10, Train Loss: 0.026185, Test Loss: 0.000829\n",
            "Config 2, Epoch 2/10, Train Loss: 0.000754, Test Loss: 0.001216\n",
            "Config 2, Epoch 3/10, Train Loss: 0.000600, Test Loss: 0.003452\n",
            "Config 2, Epoch 4/10, Train Loss: 0.000528, Test Loss: 0.007747\n",
            "Config 2, Epoch 5/10, Train Loss: 0.000538, Test Loss: 0.001890\n",
            "Config 2, Epoch 6/10, Train Loss: 0.000535, Test Loss: 0.000677\n",
            "Config 2, Epoch 7/10, Train Loss: 0.000476, Test Loss: 0.001321\n",
            "Config 2, Epoch 8/10, Train Loss: 0.000509, Test Loss: 0.000431\n",
            "Config 2, Epoch 9/10, Train Loss: 0.000490, Test Loss: 0.006782\n",
            "Config 2, Epoch 10/10, Train Loss: 0.000514, Test Loss: 0.000852\n",
            "Model for config 2 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/stock_predictor_model_config_1.pth\n",
            "Training history for config 2 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/training_history_config_1.csv\n",
            "\n",
            "Training with config 3/100: {'batch_size': 64, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.0, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'num_epochs': 20}\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/checkpoint_best_config_2.pth has incompatible hyperparameters: {'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.25}\n",
            "Current config: {'batch_size': 64, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.0, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'num_epochs': 20}\n",
            "Initializing model with random weights for config 2\n",
            "Training model with 20 epochs\n",
            "Config 3, Epoch 1/20, Train Loss: 0.013458, Test Loss: 0.002078\n",
            "Config 3, Epoch 2/20, Train Loss: 0.001047, Test Loss: 0.007308\n",
            "Config 3, Epoch 3/20, Train Loss: 0.000888, Test Loss: 0.009032\n",
            "Config 3, Epoch 4/20, Train Loss: 0.000525, Test Loss: 0.011151\n",
            "Config 3, Epoch 5/20, Train Loss: 0.000481, Test Loss: 0.019909\n",
            "Config 3, Epoch 6/20, Train Loss: 0.000437, Test Loss: 0.001174\n",
            "Config 3, Epoch 7/20, Train Loss: 0.000438, Test Loss: 0.038063\n",
            "Config 3, Epoch 8/20, Train Loss: 0.000627, Test Loss: 0.000651\n",
            "Config 3, Epoch 9/20, Train Loss: 0.000395, Test Loss: 0.000521\n",
            "Config 3, Epoch 10/20, Train Loss: 0.000269, Test Loss: 0.000630\n",
            "Config 3, Epoch 11/20, Train Loss: 0.000282, Test Loss: 0.001107\n",
            "Config 3, Epoch 12/20, Train Loss: 0.000333, Test Loss: 0.006298\n",
            "Config 3, Epoch 13/20, Train Loss: 0.000335, Test Loss: 0.000560\n",
            "Config 3, Epoch 14/20, Train Loss: 0.000271, Test Loss: 0.005300\n",
            "Config 3, Epoch 15/20, Train Loss: 0.000268, Test Loss: 0.009372\n",
            "Config 3, Epoch 16/20, Train Loss: 0.000275, Test Loss: 0.002027\n",
            "Config 3, Epoch 17/20, Train Loss: 0.000277, Test Loss: 0.003414\n",
            "Config 3, Epoch 18/20, Train Loss: 0.000263, Test Loss: 0.000615\n",
            "Config 3, Epoch 19/20, Train Loss: 0.000257, Test Loss: 0.001021\n",
            "Config 3, Epoch 20/20, Train Loss: 0.000242, Test Loss: 0.000419\n",
            "Model for config 3 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/stock_predictor_model_config_2.pth\n",
            "Training history for config 3 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/training_history_config_2.csv\n",
            "\n",
            "Training with config 4/100: {'batch_size': 16, 'hidden_size': 32, 'num_layers': 2, 'dropout_rate': 0.0, 'learning_rate': 0.0001, 'weight_decay': 0.0001, 'num_epochs': 20}\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/checkpoint_best_config_3.pth has incompatible hyperparameters: {'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.25}\n",
            "Current config: {'batch_size': 16, 'hidden_size': 32, 'num_layers': 2, 'dropout_rate': 0.0, 'learning_rate': 0.0001, 'weight_decay': 0.0001, 'num_epochs': 20}\n",
            "Initializing model with random weights for config 3\n",
            "Training model with 20 epochs\n",
            "Config 4, Epoch 1/20, Train Loss: 0.009001, Test Loss: 0.007577\n",
            "Config 4, Epoch 2/20, Train Loss: 0.001259, Test Loss: 0.006538\n",
            "Config 4, Epoch 3/20, Train Loss: 0.000990, Test Loss: 0.003021\n",
            "Config 4, Epoch 4/20, Train Loss: 0.000734, Test Loss: 0.007853\n",
            "Config 4, Epoch 5/20, Train Loss: 0.000646, Test Loss: 0.005729\n",
            "Config 4, Epoch 6/20, Train Loss: 0.000594, Test Loss: 0.005026\n",
            "Config 4, Epoch 7/20, Train Loss: 0.000549, Test Loss: 0.001294\n",
            "Config 4, Epoch 8/20, Train Loss: 0.000440, Test Loss: 0.001642\n",
            "Config 4, Epoch 9/20, Train Loss: 0.000466, Test Loss: 0.004511\n",
            "Config 4, Epoch 10/20, Train Loss: 0.000469, Test Loss: 0.007627\n",
            "Config 4, Epoch 11/20, Train Loss: 0.000453, Test Loss: 0.002288\n",
            "Config 4, Epoch 12/20, Train Loss: 0.000408, Test Loss: 0.004884\n",
            "Config 4, Epoch 13/20, Train Loss: 0.000398, Test Loss: 0.002609\n",
            "Config 4, Epoch 14/20, Train Loss: 0.000381, Test Loss: 0.003848\n",
            "Config 4, Epoch 15/20, Train Loss: 0.000363, Test Loss: 0.004984\n",
            "Config 4, Epoch 16/20, Train Loss: 0.000344, Test Loss: 0.005737\n",
            "Config 4, Epoch 17/20, Train Loss: 0.000383, Test Loss: 0.001003\n",
            "Config 4, Epoch 18/20, Train Loss: 0.000347, Test Loss: 0.003844\n",
            "Config 4, Epoch 19/20, Train Loss: 0.000349, Test Loss: 0.002389\n",
            "Config 4, Epoch 20/20, Train Loss: 0.000358, Test Loss: 0.002607\n",
            "Model for config 4 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/stock_predictor_model_config_3.pth\n",
            "Training history for config 4 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/training_history_config_3.csv\n",
            "\n",
            "Training with config 5/100: {'batch_size': 32, 'hidden_size': 32, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.0001, 'weight_decay': 0.0, 'num_epochs': 10}\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/checkpoint_best_config_4.pth has incompatible hyperparameters: {'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.1}\n",
            "Current config: {'batch_size': 32, 'hidden_size': 32, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.0001, 'weight_decay': 0.0, 'num_epochs': 10}\n",
            "Initializing model with random weights for config 4\n",
            "Training model with 10 epochs\n",
            "Config 5, Epoch 1/10, Train Loss: 0.045322, Test Loss: 0.012130\n",
            "Config 5, Epoch 2/10, Train Loss: 0.006211, Test Loss: 0.005184\n",
            "Config 5, Epoch 3/10, Train Loss: 0.002868, Test Loss: 0.001968\n",
            "Config 5, Epoch 4/10, Train Loss: 0.001858, Test Loss: 0.001559\n",
            "Config 5, Epoch 5/10, Train Loss: 0.001281, Test Loss: 0.001685\n",
            "Config 5, Epoch 6/10, Train Loss: 0.001012, Test Loss: 0.003085\n",
            "Config 5, Epoch 7/10, Train Loss: 0.000801, Test Loss: 0.001754\n",
            "Config 5, Epoch 8/10, Train Loss: 0.000729, Test Loss: 0.001248\n",
            "Config 5, Epoch 9/10, Train Loss: 0.000634, Test Loss: 0.000964\n",
            "Config 5, Epoch 10/10, Train Loss: 0.000571, Test Loss: 0.000971\n",
            "Model for config 5 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/stock_predictor_model_config_4.pth\n",
            "Training history for config 5 saved to /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/training_history_config_4.csv\n",
            "\n",
            "Training with config 6/100: {'batch_size': 64, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.001, 'num_epochs': 20}\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/AAPL price predictor/checkpoint_best_config_5.pth has incompatible hyperparameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.25}\n",
            "Current config: {'batch_size': 64, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.001, 'num_epochs': 20}\n",
            "Initializing model with random weights for config 5\n",
            "Training model with 20 epochs\n",
            "Config 6, Epoch 1/20, Train Loss: 0.043373, Test Loss: 0.018197\n",
            "Config 6, Epoch 2/20, Train Loss: 0.004833, Test Loss: 0.012851\n",
            "Config 6, Epoch 3/20, Train Loss: 0.001361, Test Loss: 0.003166\n",
            "Config 6, Epoch 4/20, Train Loss: 0.000551, Test Loss: 0.001724\n",
            "Config 6, Epoch 5/20, Train Loss: 0.000273, Test Loss: 0.000502\n",
            "Config 6, Epoch 6/20, Train Loss: 0.000201, Test Loss: 0.000839\n",
            "Config 6, Epoch 7/20, Train Loss: 0.000151, Test Loss: 0.001779\n",
            "Config 6, Epoch 8/20, Train Loss: 0.000123, Test Loss: 0.001022\n",
            "Config 6, Epoch 9/20, Train Loss: 0.000138, Test Loss: 0.000764\n",
            "Config 6, Epoch 10/20, Train Loss: 0.000113, Test Loss: 0.000820\n",
            "Config 6, Epoch 11/20, Train Loss: 0.000126, Test Loss: 0.000632\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c6837ae8ea4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import json  # ADDED: For saving/loading hyperparameter metadata\n",
        "\n",
        "# Load prepared data\n",
        "X_train = np.load('X_train.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "# Define custom dataset class\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx].unsqueeze(-1)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = StockDataset(X_train, y_train)\n",
        "test_dataset = StockDataset(X_test, y_test)\n",
        "\n",
        "def create_data_loaders(batch_size):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Define model architecture\n",
        "class StockPredictor(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, dropout_rate):\n",
        "        super(StockPredictor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.bn = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.bn(out[:, -1, :])\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Define hyperparameter search grid and sampling function\n",
        "hyperparam_grid = {\n",
        "    'batch_size': [16, 32, 64, 128],\n",
        "    'hidden_size': [32, 64, 128, 256],\n",
        "    'num_layers': [1, 2, 3],\n",
        "    'dropout_rate': [0.0, 0.1, 0.25, 0.5],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'weight_decay': [0.0, 0.0001, 0.001],\n",
        "    'num_epochs': [10, 20]\n",
        "}\n",
        "\n",
        "def sample_hyperparameters():\n",
        "    \"\"\"Randomly sample a set of hyperparameters from the grid.\"\"\"\n",
        "    return {\n",
        "        'batch_size': random.choice(hyperparam_grid['batch_size']),\n",
        "        'hidden_size': random.choice(hyperparam_grid['hidden_size']),\n",
        "        'num_layers': random.choice(hyperparam_grid['num_layers']),\n",
        "        'dropout_rate': random.choice(hyperparam_grid['dropout_rate']),\n",
        "        'learning_rate': random.choice(hyperparam_grid['learning_rate']),\n",
        "        'weight_decay': random.choice(hyperparam_grid['weight_decay']),\n",
        "        'num_epochs': random.choice(hyperparam_grid['num_epochs'])\n",
        "    }\n",
        "\n",
        "# Flag to enable/disable hyperparameter search\n",
        "USE_HYPERPARAM_SEARCH = True  # Set to True for the error case\n",
        "NUM_SEARCH_ITERATIONS = 100  # As per the error (2 configs)\n",
        "\n",
        "# Initialize best configuration tracking\n",
        "best_config = None\n",
        "best_test_loss = float('inf')\n",
        "best_config_path = os.path.join(notebook_dir, 'best_config.txt')\n",
        "\n",
        "# Conditional configuration setup\n",
        "if not USE_HYPERPARAM_SEARCH:\n",
        "    config = {\n",
        "        'batch_size': 32,\n",
        "        'hidden_size': 64,\n",
        "        'num_layers': 2,\n",
        "        'dropout_rate': 0.25,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 0.001,\n",
        "        'num_epochs': 50\n",
        "    }\n",
        "    configs_to_try = [config]\n",
        "else:\n",
        "    configs_to_try = [sample_hyperparameters() for _ in range(NUM_SEARCH_ITERATIONS)]\n",
        "    print(f\"Performing hyperparameter search over {NUM_SEARCH_ITERATIONS} random configurations\")\n",
        "\n",
        "# Training loop over configurations\n",
        "for config_idx, config in enumerate(configs_to_try):\n",
        "    print(f\"\\nTraining with config {config_idx + 1}/{len(configs_to_try)}: {config}\")\n",
        "\n",
        "    # Create data loaders with current batch size\n",
        "    train_loader, test_loader = create_data_loaders(config['batch_size'])\n",
        "\n",
        "    # Initialize model with current parameters\n",
        "    model = StockPredictor(\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout_rate=config['dropout_rate']\n",
        "    )\n",
        "\n",
        "    # MODIFIED: Load saved weights if available and compatible\n",
        "    checkpoint_path = os.path.join(notebook_dir, f'checkpoint_best_config_{config_idx}.pth')\n",
        "    best_loss_path = os.path.join(notebook_dir, f'best_loss_config_{config_idx}.txt')\n",
        "    metadata_path = os.path.join(notebook_dir, f'config_metadata_{config_idx}.json')  # ADDED: Metadata file\n",
        "\n",
        "    current_best_loss = float('inf')\n",
        "    can_load_checkpoint = False\n",
        "    if os.path.exists(checkpoint_path) and os.path.exists(metadata_path):\n",
        "        # ADDED: Load metadata and check compatibility\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            saved_config = json.load(f)\n",
        "        # Check if critical hyperparameters match\n",
        "        if (saved_config['hidden_size'] == config['hidden_size'] and\n",
        "            saved_config['num_layers'] == config['num_layers'] and\n",
        "            saved_config['dropout_rate'] == config['dropout_rate']):\n",
        "            can_load_checkpoint = True\n",
        "            try:\n",
        "                model.load_state_dict(torch.load(checkpoint_path))\n",
        "                print(f\"Loaded weights from {checkpoint_path}\")\n",
        "                if os.path.exists(best_loss_path):\n",
        "                    with open(best_loss_path, 'r') as f:\n",
        "                        current_best_loss = float(f.read())\n",
        "                    print(f\"Loaded best loss for config {config_idx}: {current_best_loss}\")\n",
        "                else:\n",
        "                    print(f\"No best loss file found for config {config_idx}, starting with infinity\")\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Failed to load checkpoint due to architecture mismatch: {e}\")\n",
        "                can_load_checkpoint = False\n",
        "        else:\n",
        "            print(f\"Checkpoint {checkpoint_path} has incompatible hyperparameters: {saved_config}\")\n",
        "            print(f\"Current config: {config}\")\n",
        "    else:\n",
        "        print(f\"No checkpoint or metadata found for config {config_idx}, starting with random weights\")\n",
        "\n",
        "    if not can_load_checkpoint:\n",
        "        print(f\"Initializing model with random weights for config {config_idx}\")\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Training loop with progress monitoring\n",
        "    print(f\"Training model with {config['num_epochs']} epochs\")\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            X_batch, y_batch = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        total_test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                X_batch, y_batch = batch\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        test_losses.append(avg_test_loss)\n",
        "\n",
        "        print(f\"Config {config_idx + 1}, Epoch {epoch + 1}/{config['num_epochs']}, Train Loss: {avg_train_loss:.6f}, Test Loss: {avg_test_loss:.6f}\")\n",
        "\n",
        "        # MODIFIED: Save weights and metadata if this epoch has the best test loss\n",
        "        if avg_test_loss < current_best_loss:\n",
        "            current_best_loss = avg_test_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            with open(best_loss_path, 'w') as f:\n",
        "                f.write(str(current_best_loss))\n",
        "            # ADDED: Save hyperparameter metadata\n",
        "            with open(metadata_path, 'w') as f:\n",
        "                json.dump({\n",
        "                    'hidden_size': config['hidden_size'],\n",
        "                    'num_layers': config['num_layers'],\n",
        "                    'dropout_rate': config['dropout_rate']\n",
        "                }, f)\n",
        "            # print(f\"Saved new best weights for config {config_idx + 1} with test loss {current_best_loss}\")\n",
        "\n",
        "        # Update global best configuration\n",
        "        if avg_test_loss < best_test_loss:\n",
        "            best_test_loss = avg_test_loss\n",
        "            best_config = config\n",
        "            with open(best_config_path, 'w') as f:\n",
        "                f.write(str(best_config))\n",
        "\n",
        "    # Save trained model for this config\n",
        "    torch.save(model.state_dict(), os.path.join(notebook_dir, f'stock_predictor_model_config_{config_idx}.pth'))\n",
        "    print(f\"Model for config {config_idx + 1} saved to {os.path.join(notebook_dir, f'stock_predictor_model_config_{config_idx}.pth')}\")\n",
        "\n",
        "    # Save training history for this config\n",
        "    history_df = pd.DataFrame({'epoch': range(1, config['num_epochs'] + 1), 'train_loss': train_losses, 'test_loss': test_losses})\n",
        "    history_df.to_csv(os.path.join(notebook_dir, f'training_history_config_{config_idx}.csv'), index=False)\n",
        "    print(f\"Training history for config {config_idx + 1} saved to {os.path.join(notebook_dir, f'training_history_config_{config_idx}.csv')}\")\n",
        "\n",
        "# Print and load the best configuration\n",
        "if USE_HYPERPARAM_SEARCH and best_config is not None:\n",
        "    print(f\"\\nBest configuration found: {best_config} with test loss {best_test_loss}\")\n",
        "    best_config_idx = configs_to_try.index(best_config) if best_config in configs_to_try else 0\n",
        "    model.load_state_dict(torch.load(os.path.join(notebook_dir, f'checkpoint_best_config_{best_config_idx}.pth')))\n",
        "    print(f\"Loaded best model weights for prediction from config {best_config_idx + 1}\")\n",
        "else:\n",
        "    model.load_state_dict(torch.load(os.path.join(notebook_dir, 'checkpoint_best_config_0.pth')))\n",
        "    print(\"Using weights from the fixed configuration\")\n",
        "\n",
        "# Rest of the code remains unchanged\n",
        "def prepare_latest_sequence(df, scaler_X, feature_columns, sequence_length=90):\n",
        "    \"\"\"Prepare the latest sequence for prediction.\"\"\"\n",
        "    latest_data = df.iloc[-sequence_length:].copy().reset_index()\n",
        "    latest_data['Date'] = pd.to_datetime(latest_data['Date'])\n",
        "    delta = latest_data['Close'].diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=14, min_periods=1).mean()\n",
        "    avg_loss = loss.rolling(window=14, min_periods=1).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    latest_data['RSI_14'] = 100 - (100 / (1 + rs))\n",
        "    latest_data['Gap_Up'] = (latest_data['Open'] > latest_data['Close'].shift(1)).astype(int)\n",
        "    latest_data['GAP_DOWN'] = (latest_data['Open'] < latest_data['Close'].shift(1)).astype(int)\n",
        "    latest_data['Gap_Up'] = latest_data['Gap_Up'].fillna(0).astype(int)\n",
        "    latest_data['GAP_DOWN'] = latest_data['GAP_DOWN'].fillna(0).astype(int)\n",
        "    latest_data['DAY_OF_WEEK'] = latest_data['Date'].dt.dayofweek\n",
        "    latest_data['MONTH'] = latest_data['Date'].dt.month\n",
        "    latest_data['DAY_OF_MONTH'] = latest_data['Date'].dt.day\n",
        "    latest_data = latest_data.drop(['Next_Day_Open'], axis=1, errors='ignore')\n",
        "    missing_cols = [col for col in feature_columns if col not in latest_data.columns]\n",
        "    for col in missing_cols:\n",
        "        latest_data[col] = 0\n",
        "    features = latest_data[feature_columns]\n",
        "    if features.isnull().any().any():\n",
        "        features = features.fillna(0)\n",
        "    scaled_features = scaler_X.transform(features)\n",
        "    return torch.tensor(scaled_features, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "def make_and_store_prediction(model, df, scaler_X, scaler_y, feature_columns):\n",
        "    \"\"\"Make prediction for next day's open and store results.\"\"\"\n",
        "    prediction_file = os.path.join(notebook_dir, 'predictions.csv')\n",
        "    print(f\"Attempting to save predictions to: {prediction_file}\")\n",
        "    latest_sequence = prepare_latest_sequence(df, scaler_X, feature_columns)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scaled_prediction = model(latest_sequence).numpy()\n",
        "        predicted_price = scaler_y.inverse_transform(scaled_prediction)[0][0]\n",
        "    current_date = df.index[-1].date()\n",
        "    prev_close = df['Close'].iloc[-1]\n",
        "    next_day = current_date + timedelta(days=1)\n",
        "    predicted_change = ((predicted_price - prev_close) / prev_close) * 100\n",
        "    actual_price = df['Open'].iloc[-1] if 'Open' in df.columns else np.nan\n",
        "    actual_change = ((actual_price - prev_close) / prev_close) * 100 if not np.isnan(actual_price) else np.nan\n",
        "    prediction_record = {\n",
        "        'Date': current_date,\n",
        "        'Previous_Close': prev_close,\n",
        "        'Predicted_Price': predicted_price,\n",
        "        'Predicted_Change_Percent': predicted_change,\n",
        "        'Actual_Price': actual_price,\n",
        "        'Actual_Change_Percent': actual_change\n",
        "    }\n",
        "    dtypes = {\n",
        "        'Date': 'datetime64[ns]',\n",
        "        'Previous_Close': 'float64',\n",
        "        'Predicted_Price': 'float64',\n",
        "        'Predicted_Change_Percent': 'float64',\n",
        "        'Actual_Price': 'float64',\n",
        "        'Actual_Change_Percent': 'float64'\n",
        "    }\n",
        "    if os.path.exists(prediction_file):\n",
        "        pred_df = pd.read_csv(prediction_file, parse_dates=['Date'])\n",
        "    else:\n",
        "        pred_df = pd.DataFrame(columns=['Date', 'Previous_Close', 'Predicted_Price',\n",
        "                                        'Predicted_Change_Percent', 'Actual_Price',\n",
        "                                        'Actual_Change_Percent']).astype(dtypes)\n",
        "    new_record_df = pd.DataFrame([prediction_record]).astype(dtypes)\n",
        "    if current_date not in pred_df['Date'].values:\n",
        "        pred_df = pd.concat([pred_df, new_record_df], ignore_index=True)\n",
        "        try:\n",
        "            pred_df.to_csv(prediction_file, index=False)\n",
        "            print(f\"Prediction successfully saved to {prediction_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving predictions to {prediction_file}: {e}\")\n",
        "    else:\n",
        "        print(f\"Prediction for {current_date} already exists\")\n",
        "    return predicted_price, next_day\n",
        "\n",
        "# Make prediction for next day\n",
        "print(\"\\nMaking prediction for next day's opening price...\")\n",
        "feature_columns = df_clean.drop(['Next_Day_Open'], axis=1).columns\n",
        "predicted_price, next_day = make_and_store_prediction(model, aapl_data, scaler_X, scaler_y, feature_columns)\n",
        "print(f\"Predicted next day opening price for {next_day}: ${predicted_price:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhjtiOltix6U"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPn4orVLEEDh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNwXo9m5km5MjOqkBxkrpy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}